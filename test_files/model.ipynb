{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, temperature = 10000):\n",
    "        '''\n",
    "        d_model: feature dimension (default = 768)\n",
    "        seq_len: sequence length\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.temperature = temperature\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        pos = torch.arange(self.seq_len, dtype=torch.float32).unsqueeze(1)              # pos = [[0], [1], ..., [seq_len-1]]\n",
    "        i = torch.arange(self.d_model // 2, dtype=torch.float32).unsqueeze(0)           # i = [[0, 1, ..., d_model/2 - 1]]\n",
    "\n",
    "        # Compute the positional encodings\n",
    "        angle_rates = 1 / (self.temperature ** (2 * i / self.d_model))\n",
    "        pos_encoding = torch.zeros(self.seq_len, self.d_model, dtype=torch.float32)\n",
    "        pos_encoding[:, 0::2] = torch.sin(pos * angle_rates)\n",
    "        pos_encoding[:, 1::2] = torch.cos(pos * angle_rates)\n",
    "\n",
    "        # Add a dimension for batch size\n",
    "        pos_encoding = pos_encoding.unsqueeze(0)\n",
    "\n",
    "        # Disable gradient because PE are not learnable parameters\n",
    "        pos_encoding.requires_grad_(False)\n",
    "\n",
    "        return pos_encoding     # pos_encoding = [1, seq_len, 768]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hynguyen/opt/anaconda3/envs/torch-gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTModel\n",
    "from transformers import ViTImageProcessor\n",
    "import math\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, frames):\n",
    "        '''\n",
    "        frames: sequence of PIL Image\n",
    "        input_embed: (batch, seq_len, d_model)\n",
    "        '''\n",
    "        model_name = 'google/vit-base-patch16-224'\n",
    "        model = ViTModel.from_pretrained(model_name)\n",
    "        processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "        input_embed = []\n",
    "        for frame in frames:\n",
    "            inputs = processor(images=frame, return_tensors='pt')\n",
    "            pixel_values = inputs.pixel_values                      # pixel_values = [1, 3, 224, 224]\n",
    "            with torch.no_grad():\n",
    "                output = model(pixel_values)\n",
    "                output = output.last_hidden_state[:, 0]             # Get the [CLS] output, shape = [1, 768]\n",
    "                input_embed.append(output)\n",
    "        input_embed = torch.cat(input_embed, dim=0)                 # input_embed = [seq_len, 768]\n",
    "        input_embed = input_embed.unsqueeze(0)                      # input_embed = [1, seq_len, 768] (Add batch dimension)\n",
    "        d_model = input_embed.shape[-1]\n",
    "        # Scale the embeddings\n",
    "        input_embed = input_embed * math.sqrt(d_model)\n",
    "        \n",
    "        return input_embed        # input_embed = [1, seq_len, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, d_model, epsilon=10**-6):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = nn.Parameter(torch.ones(d_model))      # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model))      # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "            return: normalized x (batch, seq_len, d_model)\n",
    "        '''\n",
    "        mean = x.mean(dim=-1, keepdim=True)         # (batch, seq_len, 1)\n",
    "        std = x.std(dim=-1, keepdim=True)           # (batch, seq_len, 1)\n",
    "        return self.alpha * (x-mean) / (std + self.epsilon) + self.bias     # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: (batch, seq_len, d_model)\n",
    "        '''\n",
    "        output = self.linear_1(x)       # output: (batch, seq_len, d_ff)\n",
    "        output = torch.relu(output)     \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear_2(output)  # output: (batch, seq_len, d_model)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        w_q = [nn.Linear(d_model, d_model) for _ in range(num_heads)]      # w_q = (num_heads, d_model, d_model)\n",
    "        w_k = [nn.Linear(d_model, d_model) for _ in range(num_heads)]      # w_k = (num_heads, d_model, d_model)\n",
    "        w_v = [nn.Linear(d_model, d_model) for _ in range(num_heads)]      # w_v = (num_heads, d_model, d_model)\n",
    "        \n",
    "        self.w_q = nn.ModuleList(w_q)\n",
    "        self.w_k = nn.ModuleList(w_k)\n",
    "        self.w_v = nn.ModuleList(w_v)\n",
    "        self.w_o = nn.Linear(num_heads * d_model, d_model, bias=False)      # w_o = (num_heads * d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: (batch, seq_len, d_model)\n",
    "        '''\n",
    "        q = [layer(x) for layer in self.w_q]        # q[i] = (batch, seq_len, d_model)\n",
    "        k = [layer(x) for layer in self.w_k]        # k[i] = (batch, seq_len, d_model)\n",
    "        v = [layer(x) for layer in self.w_v]        # v[i] = (batch, seq_len, d_model)\n",
    "        q, k, v = torch.stack(q), torch.stack(k), torch.stack(v)        # q, k, v = (num_heads, batch, seq_len, d_model)\n",
    "        q, k, v = q.permute(1, 0, 2, 3)                                 # q, k, v = (batch, num_heads, seq_len, d_model)\n",
    "\n",
    "        k_transpose = k.transpose(-2, -1)           # k_transpose = (batch, num_heads, d_model, seq_len)\n",
    "        attention_scores = q @ k_transpose          # attention_score = (batch, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # Normalise the attention scores\n",
    "        attention_scores = attention_scores / self.d_model      # attention_scores = (batch, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # Apply softmax to attention scores\n",
    "        attention_scores = attention_scores.softmax(dim=-1)     # attention_scores = (batch, num_heads, seq_len, seq_len)\n",
    "\n",
    "        # Dropout\n",
    "        if self.dropout is not None:\n",
    "            attention_scores = self.dropout(attention_scores)   # attention_scores = (batch, num_heads, seq_len, seq_len)\n",
    "\n",
    "        # Calculate all heads\n",
    "        heads = attention_scores @ v                            # heads = (batch, num_heads, seq_len, d_model)\n",
    "\n",
    "        # Concatenate heads along the seq_len dimension\n",
    "        heads = heads.transpose(1, 2)                                           # heads = (batch, seq_len, num_heads, d_model)\n",
    "        heads = heads.contiguous().view(heads.shape[0], heads.shape[1], -1)     # heads = (batch, seq_len, num_heads * d_model)\n",
    "\n",
    "        # Linear transform with output weights\n",
    "        output = self.w_o(heads)                                # output = (batch, seq_len, d_model)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNormBlock(nn.Module):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(d_model)\n",
    "    \n",
    "    def forward(self, x, sublayer, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Apply residual connection to any sublayer with the same size.\n",
    "        x: Input tensor\n",
    "        sublayer: A function representing the sublayer (e.g., multi-head attention, feed-forward)\n",
    "        args: Additional positional arguments for the sublayer\n",
    "        kwargs: Additional keyword arguments for the sublayer\n",
    "        \"\"\"\n",
    "        return self.norm(x + self.dropout(sublayer(x, *args, **kwargs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, mhsa_block: MultiHeadAttentionBlock, \n",
    "                 feed_forward_block: FeedForwardBlock, \n",
    "                 d_model: int,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "        self.mhsa_block = mhsa_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.add_norm_block = nn.ModuleList([AddNormBlock(d_model, dropout) for _ in range(2)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "            x: input [batch, seq_len, d_model]\n",
    "        '''\n",
    "        x = self.add_norm_block[0](x, lambda x: self.mhsa_block(x))\n",
    "        x = self.add_norm_block[1](x, self.feed_forward_block)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model: int, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionLayer(nn.Module):\n",
    "    def __init__(self, d_model, dropout, vit_num_features=197, d1=512, d2=256):\n",
    "        '''\n",
    "        Args:\n",
    "            d_model: feature dimension of an input embedding\n",
    "            vit_num_features: the number of features produced by ViT model\n",
    "            d1, d2: dimensions of the two hidden layers in PredictionLayer\n",
    "\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d1)\n",
    "        self.fc2 = nn.Linear(d1, d2)\n",
    "        self.fc3 = nn.Linear(d2, vit_num_features * d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        '''\n",
    "        x = self.relu(self.fc1(x))      # (batch, seq_len, d_model) x (d_model, d1) = (batch, seq_len, d1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))      # (batch, seq_len, d1) x (d1, d2) = (batch, seq_len, d2)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)                 # (batch, seq_len, d2) x (d2, vit_num_features * d_model) = (batch, seq_len, vit_num_features * d_model)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCPModel(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, pred_layer: PredictionLayer, src_embed: InputEmbeddings, src_pos: PositionalEncoding):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.pred_layer = pred_layer\n",
    "        self.src_embed = src_embed\n",
    "        self.src_pos = src_pos\n",
    "\n",
    "    def forward(self, src):\n",
    "        '''\n",
    "        Args:\n",
    "            src: 'n' frames\n",
    "        '''\n",
    "\n",
    "        # Get input embedding\n",
    "        src = self.src_embed(src)           # src = (batch, seq_len, d_model)\n",
    "\n",
    "        # Get positional encoding\n",
    "        pos_encoding = self.src_pos()       # pos_encoding = (batch, seq_len, d_model)\n",
    "\n",
    "        # Add input embedding + positional encoding to generate the complete input\n",
    "        input = src + pos_encoding          # input = (batch, seq_len, d_model)\n",
    "\n",
    "        # Get output from the encoder module\n",
    "        output = self.encoder(input)        # output = (batch, seq_len, d_model)\n",
    "\n",
    "        # Get output from the prediction layer module\n",
    "        output = self.pred_layer(output)    # output = (batch, embed_num_features * d_model)\n",
    "\n",
    "        # Pass the complete input into Encoder and return the result\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(d_model, seq_len, N = 6, h = 8, dropout = 0.1, d_ff = 2048):\n",
    "    '''\n",
    "    d_model: feature dimension of an input embedding\n",
    "    seq_len: length of the input sequence\n",
    "    N: number of encoder blocks in the model\n",
    "    h: number of heads for multi-head self-attention\n",
    "    d_ff: the dimension of the hidden layer of Feed Forward Block\n",
    "    '''\n",
    "    # Input embedding layer\n",
    "    src_embed = InputEmbeddings()\n",
    "    \n",
    "    # Positional encoding layer\n",
    "    pos_enc = PositionalEncoding(d_model, seq_len)\n",
    "\n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        mhsa_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(mhsa_block, feed_forward_block, d_model, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "    \n",
    "    # Create the encoder \n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "\n",
    "    # Create the prediction layer\n",
    "    pred_layer = PredictionLayer(d_model, dropout)\n",
    "\n",
    "    # Create the Semantic Concentration Encoder\n",
    "    model = SCPModel(encoder, pred_layer, src_embed, pos_enc)\n",
    "\n",
    "    # Initialise the parameters of the model\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 197, 2])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "batch = 1\n",
    "d_model = 2  # Example value\n",
    "seq_len = 3  # Example value\n",
    "\n",
    "input = torch.rand(batch, seq_len, d_model)\n",
    "\n",
    "fcn = YourModel(d_model, seq_len)\n",
    "\n",
    "output = fcn(input)\n",
    "\n",
    "print(output.shape)  # Should print torch.Size([32, 1, 197, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0715, 0.5544, 0.3741, 0.0000],\n",
       "         [0.0026, 0.0000, 0.0000, 0.9974],\n",
       "         [0.2822, 0.1855, 0.3516, 0.1807],\n",
       "         [0.8689, 0.0290, 0.1021, 0.0000]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 1.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 1.0000, 0.0000],\n",
       "         [0.9349, 0.0000, 0.0000, 0.0651]],\n",
       "\n",
       "        [[0.0000, 0.0000, 1.0000, 0.0000],\n",
       "         [0.3121, 0.0000, 0.1527, 0.5352],\n",
       "         [0.0000, 0.0000, 0.0000, 1.0000],\n",
       "         [0.2662, 0.0000, 0.7338, 0.0000]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads, seq_len, d_model = 3, 4, 5\n",
    "\n",
    "# Initialize q and k with random values\n",
    "q = torch.randn(num_heads, seq_len, d_model)\n",
    "k = torch.randn(num_heads, seq_len, d_model)\n",
    "v = torch.randn(num_heads, seq_len, d_model)\n",
    "\n",
    "k_transpose = k.transpose(-2, -1) \n",
    "\n",
    "result = q @ k_transpose\n",
    "result = result / 2\n",
    "\n",
    "masks = torch.randint(0, 2, (3, 4, 4))\n",
    "result = torch.where(masks == 1, result, torch.tensor(float('-inf')))\n",
    "result = result.softmax(dim=-1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1430, 1.1089, 0.7482, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 1.9948],\n",
       "         [0.0000, 0.0000, 0.7032, 0.3614],\n",
       "         [0.0000, 0.0000, 0.2042, 0.0000]],\n",
       "\n",
       "        [[2.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 2.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 2.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.3054, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5324, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = nn.Dropout(0.5)\n",
    "result = dropout(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([10, 1024])\n"
     ]
    }
   ],
   "source": [
    "d_model = 128\n",
    "num_heads = 8\n",
    "dropout = 0.1\n",
    "attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "\n",
    "seq_len = 10\n",
    "x = torch.rand(seq_len, d_model)\n",
    "masks = torch.ones(num_heads, seq_len, seq_len)  # Adjust masks as necessary for your tests\n",
    "\n",
    "output = attention_block(x, masks)\n",
    "expected_shape = (seq_len, num_heads * d_model)\n",
    "\n",
    "print(type(output))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 480, 640])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision import transforms\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "images = []\n",
    "transform = transforms.ToTensor()\n",
    "for _ in range(4):\n",
    "    im = Image.open(requests.get(url, stream=True).raw)\n",
    "    im = transform(im)\n",
    "    images.append(im)\n",
    "\n",
    "tensor_images = torch.stack(images)\n",
    "tensor_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "IE = InputEmbeddings()\n",
    "input = IE(tensor_images)\n",
    "_, seq_len, d_model = input.shape\n",
    "\n",
    "PE = PositionalEncoding(d_model, seq_len)\n",
    "pe = PE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = input + pe.to(device)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = FeedForwardBlock(d_model, 200, 0.1).to(device)\n",
    "output = ff(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
