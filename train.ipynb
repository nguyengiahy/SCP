{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T12:04:38.612042Z","iopub.status.busy":"2024-03-11T12:04:38.611176Z","iopub.status.idle":"2024-03-11T12:05:07.600006Z","shell.execute_reply":"2024-03-11T12:05:07.598694Z","shell.execute_reply.started":"2024-03-11T12:04:38.612011Z"},"id":"0Wg7ykEoDRKS","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-11 12:04:46.630423: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-11 12:04:46.630541: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-11 12:04:46.736275: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\n","Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\n","Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\n","Requirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\n","Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n"]}],"source":["#Library import\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from transformers import ViTModel\n","from transformers import ViTImageProcessor\n","import math\n","import os\n","import imageio\n","from torch.utils.tensorboard import SummaryWriter\n","from pathlib import Path\n","import logging\n","from datetime import datetime\n","!pip install tensorboard"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T12:05:07.603149Z","iopub.status.busy":"2024-03-11T12:05:07.602146Z","iopub.status.idle":"2024-03-11T12:05:20.615843Z","shell.execute_reply":"2024-03-11T12:05:20.614666Z","shell.execute_reply.started":"2024-03-11T12:05:07.603118Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: imageio[ffmpeg] in /opt/conda/lib/python3.10/site-packages (2.33.1)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from imageio[ffmpeg]) (1.26.4)\n","Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio[ffmpeg]) (9.5.0)\n","Collecting imageio-ffmpeg (from imageio[ffmpeg])\n","  Downloading imageio_ffmpeg-0.4.9-py3-none-manylinux2010_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from imageio[ffmpeg]) (5.9.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from imageio-ffmpeg->imageio[ffmpeg]) (69.0.3)\n","Downloading imageio_ffmpeg-0.4.9-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: imageio-ffmpeg\n","Successfully installed imageio-ffmpeg-0.4.9\n"]}],"source":["#Kaggle requirement\n","!pip install imageio[ffmpeg]"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T12:05:20.623079Z","iopub.status.busy":"2024-03-11T12:05:20.622552Z","iopub.status.idle":"2024-03-11T12:05:20.674507Z","shell.execute_reply":"2024-03-11T12:05:20.673665Z","shell.execute_reply.started":"2024-03-11T12:05:20.623038Z"},"id":"3bxtppvuEVFR","trusted":true},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, seq_len, temperature = 10000):\n","        '''\n","        d_model: feature dimension (default = 768)\n","        seq_len: sequence length\n","        '''\n","        super().__init__()\n","        self.d_model = d_model\n","        self.seq_len = seq_len\n","        self.temperature = temperature\n","\n","\n","    def forward(self):\n","        pos = torch.arange(self.seq_len, dtype=torch.float32).unsqueeze(1)              # pos = [[0], [1], ..., [seq_len-1]]\n","        i = torch.arange(self.d_model // 2, dtype=torch.float32).unsqueeze(0)           # i = [[0, 1, ..., d_model/2 - 1]]\n","\n","        # Compute the positional encodings\n","        angle_rates = 1 / (self.temperature ** (2 * i / self.d_model))\n","        pos_encoding = torch.zeros(self.seq_len, self.d_model, dtype=torch.float32)\n","        pos_encoding[:, 0::2] = torch.sin(pos * angle_rates)\n","        pos_encoding[:, 1::2] = torch.cos(pos * angle_rates)\n","\n","        # Add a dimension for batch size\n","        pos_encoding = pos_encoding.unsqueeze(0)\n","\n","        # Disable gradient because PE are not learnable parameters\n","        pos_encoding.requires_grad_(False)\n","\n","        return pos_encoding.to(get_device_available())     # pos_encoding = [1, seq_len, 768]\n","\n","class InputEmbeddings(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        model_name = 'google/vit-base-patch16-224'\n","        self.emb_model = ViTModel.from_pretrained(model_name)\n","        self.emb_processor = ViTImageProcessor.from_pretrained(model_name)\n","        self.emb_model.to(get_device_available())\n","\n","    def forward(self, frames):\n","        '''\n","        frames: sequence of PIL Image (batch, seq_len, channel, width, height)\n","        input_embed: (batch, seq_len, d_model)\n","        '''\n","        #input_embeds holds embedded frames of the batch (input_embeds = [batch, seq_len, d_model])\n","        input_embeds = []\n","        for i in range(frames.size(dim=0)):\n","          #input_embed holds embedded frames of one sample\n","          input_embed = []\n","          single_sample = frames[i]\n","          for frame in single_sample:\n","              #Embed frames\n","              inputs = self.emb_processor(images=frame, return_tensors='pt')\n","              pixel_values = inputs.pixel_values.to(get_device_available())                      # pixel_values = [1, 3, 224, 224]\n","              with torch.no_grad():\n","                  output = self.emb_model(pixel_values)\n","                  # Get the representation of the entire frame\n","                  output = output.last_hidden_state.mean(dim=1)       # shape = [d_model]\n","                  input_embed.append(output)\n","          input_embed = torch.cat(input_embed, dim=0)                 # input_embed = [seq_len, d_model]\n","          input_embeds.append(input_embed)\n","        input_embeds = torch.stack(input_embeds)\n","        d_model = input_embed.shape[-1]\n","        # Scale the embeddings\n","        input_embeds = input_embeds * math.sqrt(d_model)\n","\n","        return input_embeds        # input_embed = [batch, seq_len, d_model]\n","\n","class LayerNormalization(nn.Module):\n","    def __init__(self, d_model, epsilon=10**-6):\n","        super().__init__()\n","        self.epsilon = epsilon\n","        self.alpha = nn.Parameter(torch.ones(d_model))      # alpha is a learnable parameter\n","        self.bias = nn.Parameter(torch.zeros(d_model))      # bias is a learnable parameter\n","\n","    def forward(self, x):\n","        '''\n","        Args:\n","            x: (batch, seq_len, d_model)\n","            return: normalized x (batch, seq_len, d_model)\n","        '''\n","        mean = x.mean(dim=-1, keepdim=True)         # (batch, seq_len, 1)\n","        std = x.std(dim=-1, keepdim=True)           # (batch, seq_len, 1)\n","        return self.alpha * (x-mean) / (std + self.epsilon) + self.bias     # (batch, seq_len, d_model)\n","\n","class FeedForwardBlock(nn.Module):\n","    def __init__(self, d_model, d_ff, dropout):\n","        super().__init__()\n","        self.linear_1 = nn.Linear(d_model, d_ff)\n","        self.linear_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        '''\n","        x: (batch, seq_len, d_model)\n","        '''\n","        output = self.linear_1(x)       # output: (batch, seq_len, d_ff)\n","        output = torch.relu(output)\n","        output = self.dropout(output)\n","        output = self.linear_2(output)  # output: (batch, seq_len, d_model)\n","\n","        return output\n","\n","class MultiHeadAttentionBlock(nn.Module):\n","    def __init__(self, d_model, num_heads, dropout):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","\n","        w_q = [nn.Linear(d_model, d_model) for _ in range(num_heads)]      # w_q = (num_heads, d_model, d_model)\n","        w_k = [nn.Linear(d_model, d_model) for _ in range(num_heads)]      # w_k = (num_heads, d_model, d_model)\n","        w_v = [nn.Linear(d_model, d_model) for _ in range(num_heads)]      # w_v = (num_heads, d_model, d_model)\n","\n","        self.w_q = nn.ModuleList(w_q)\n","        self.w_k = nn.ModuleList(w_k)\n","        self.w_v = nn.ModuleList(w_v)\n","        self.w_o = nn.Linear(num_heads * d_model, d_model, bias=False)      # w_o = (num_heads * d_model, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        '''\n","        x: (batch, seq_len, d_model)\n","        '''\n","        q = [layer(x) for layer in self.w_q]        # q[i] = (batch, seq_len, d_model)\n","        k = [layer(x) for layer in self.w_k]        # k[i] = (batch, seq_len, d_model)\n","        v = [layer(x) for layer in self.w_v]        # v[i] = (batch, seq_len, d_model)\n","        q, k, v = torch.stack(q), torch.stack(k), torch.stack(v)        # q, k, v = (num_heads, batch, seq_len, d_model)\n","        q, k, v = q.permute(1, 0, 2, 3), k.permute(1, 0, 2, 3), v.permute(1, 0, 2, 3)                              # q, k, v = (batch, num_heads, seq_len, d_model)\n","\n","        k_transpose = k.transpose(-2, -1)           # k_transpose = (batch, num_heads, d_model, seq_len)\n","        attention_scores = q @ k_transpose          # attention_score = (batch, num_heads, seq_len, seq_len)\n","\n","        # Normalise the attention scores\n","        attention_scores = attention_scores / self.d_model      # attention_scores = (batch, num_heads, seq_len, seq_len)\n","        # Apply softmax to attention scores\n","        attention_scores = attention_scores.softmax(dim=-1)     # attention_scores = (batch, num_heads, seq_len, seq_len)\n","\n","        # Dropout\n","        if self.dropout is not None:\n","            attention_scores = self.dropout(attention_scores)   # attention_scores = (batch, num_heads, seq_len, seq_len)\n","\n","        # Calculate all heads\n","        heads = attention_scores @ v                            # heads = (batch, num_heads, seq_len, d_model)\n","\n","        # Concatenate heads along the seq_len dimension\n","        heads = heads.transpose(1, 2)                                           # heads = (batch, seq_len, num_heads, d_model)\n","        #heads_values is a copy of heads (Keep the original head before output calculation)\n","        heads_values = heads.clone()\n","        heads = heads.contiguous().view(heads.shape[0], heads.shape[1], -1)     # heads = (batch, seq_len, num_heads * d_model)\n","\n","        # Linear transform with output weights\n","        output = self.w_o(heads)                                # output = (batch, seq_len, d_model)\n","\n","        # Compute SCL loss\n","        scl_module = SCL()\n","        scl_value = scl_module(heads_values)\n","        return output, scl_value\n","\n","class AddNormBlock(nn.Module):\n","    def __init__(self, d_model, dropout):\n","        super().__init__()\n","        self.dropout = nn.Dropout(dropout)\n","        self.norm = LayerNormalization(d_model)\n","\n","    def forward(self, x, sublayer, *args, **kwargs):\n","        \"\"\"\n","        Apply residual connection to any sublayer with the same size.\n","        x: Input tensor\n","        sublayer: A function representing the sublayer (e.g., multi-head attention, feed-forward)\n","        args: Additional positional arguments for the sublayer\n","        kwargs: Additional keyword arguments for the sublayer\n","        \"\"\"\n","        output = sublayer(x)\n","        if isinstance(output, tuple):\n","            #If output not a single tensor -> MHSA (SCL loss is calculated along MHSA output)\n","            output, _ = output\n","        return self.norm(x + self.dropout(output))\n","\n","class EncoderBlock(nn.Module):\n","    def __init__(self, mhsa_block: MultiHeadAttentionBlock,\n","                 feed_forward_block: FeedForwardBlock,\n","                 d_model: int,\n","                 dropout: float):\n","        super().__init__()\n","        self.mhsa_block = mhsa_block\n","        self.feed_forward_block = feed_forward_block\n","        self.add_norm_block = nn.ModuleList([AddNormBlock(d_model, dropout) for _ in range(2)])\n","\n","    def forward(self, x):\n","        '''\n","        Args:\n","            x: input [batch, seq_len, d_model]\n","        '''\n","        #Get SCL loss for input x\n","        _, loss = self.mhsa_block(x)\n","        x = self.add_norm_block[0](x, lambda x: self.mhsa_block(x))\n","        x = self.add_norm_block[1](x, self.feed_forward_block)\n","        return x, loss\n","\n","class Encoder(nn.Module):\n","    def __init__(self, d_model: int, layers: nn.ModuleList):\n","        super().__init__()\n","        self.layers = layers\n","        self.norm = LayerNormalization(d_model)\n","\n","    def forward(self, x):\n","        loss = None               #Sum up all losses as final loss => Update whole model\n","        for layer in self.layers:\n","          if loss == None:\n","            #Init loss\n","            x, loss = layer(x)\n","          else:\n","            #Add up tmp_loss to loss\n","            x, tmp_loss = layer(x)\n","            loss += tmp_loss\n","\n","        return self.norm(x), loss\n","\n","class PredictionLayer(nn.Module):\n","    def __init__(self, d_model):\n","        '''\n","        Args:\n","            d_model: feature dimension of an input embedding\n","        '''\n","        super().__init__()\n","        self.fc = nn.Linear(d_model, d_model)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        '''\n","        Take the Transformer Encoder's output of the last frame in the sequence to predict the embedding of the next frame\n","        Args:\n","            x: (batch, seq_len, d_model)\n","            return: (batch, d_model)\n","        '''\n","        # Get the last row, which is the Attention encoded representation of the last frame\n","        x = x[:, -1, :]                 # x = (batch, d_model)\n","        x = self.relu(self.fc(x))       # x = (batch, d_model)\n","\n","        return x\n","\n","class SCPModel(nn.Module):\n","    def __init__(self, encoder: Encoder, pred_layer: PredictionLayer, src_embed: InputEmbeddings, src_pos: PositionalEncoding):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.pred_layer = pred_layer\n","        self.src_embed = src_embed\n","        self.src_pos = src_pos\n","\n","    def forward(self, src):\n","        '''\n","        Args:\n","            src: 'n' frames\n","        '''\n","\n","        # Get input embedding\n","        src = self.src_embed(src)        # src = (batch, seq_len, d_model)\n","\n","        # Get positional encoding\n","        pos_encoding = self.src_pos()       # pos_encoding = (1, seq_len, d_model)\n","\n","        # Add input embedding + positional encoding to generate the complete input\n","        input = src + pos_encoding          # input = (batch, seq_len, d_model)\n","\n","        # Get output and loss from the encoder module\n","        output, loss = self.encoder(input)        # output = (batch, seq_len, d_model), SCL_value\n","\n","        # Get output from the prediction layer module\n","        output = self.pred_layer(output)    # output = (batch, embed_num_features * d_model)\n","\n","        return output, loss\n","\n","def build_model(d_model, seq_len, N = 6, h = 8, dropout = 0.1, d_ff = 2048, device='cpu'):\n","    '''\n","    d_model: feature dimension of an input embedding\n","    seq_len: length of the input sequence\n","    N: number of encoder blocks in the model\n","    h: number of heads for multi-head self-attention\n","    d_ff: the dimension of the hidden layer of Feed Forward Block\n","    '''\n","    # Input embedding layer\n","    src_embed = InputEmbeddings().to(device)\n","\n","    # Positional encoding layer\n","    pos_enc = PositionalEncoding(d_model, seq_len).to(device)\n","\n","    # Create the encoder blocks\n","    encoder_blocks = []\n","    for _ in range(N):\n","        mhsa_block = MultiHeadAttentionBlock(d_model, h, dropout).to(device)\n","        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout).to(device)\n","        encoder_block = EncoderBlock(mhsa_block, feed_forward_block, d_model, dropout).to(device)\n","        encoder_blocks.append(encoder_block)\n","\n","    # Create the encoder\n","    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks)).to(device)\n","\n","    # Create the prediction layer\n","    pred_layer = PredictionLayer(d_model).to(device)\n","\n","    # Create the Semantic Concentration Encoder\n","    model = SCPModel(encoder, pred_layer, src_embed, pos_enc).to(device)\n","\n","    # Initialise the parameters of the model\n","    for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)\n","\n","    return model"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T12:05:20.675935Z","iopub.status.busy":"2024-03-11T12:05:20.675638Z","iopub.status.idle":"2024-03-11T12:05:20.689596Z","shell.execute_reply":"2024-03-11T12:05:20.688577Z","shell.execute_reply.started":"2024-03-11T12:05:20.675911Z"},"id":"kSODYgpCEgKu","trusted":true},"outputs":[],"source":["class MSE(nn.Module):\n","    def __init__(self, norm_dim = None):\n","        \"\"\"\n","        Args:\n","            norm_dim: the dimension that we want to normalize the ground truth and the prediction\n","        \"\"\"\n","        super().__init__()\n","        self.norm_dim = norm_dim\n","\n","    def forward(self, pred, gt):\n","        \"\"\"\n","        Args:\n","            pred: prediction made by model, tensor with shape [batch, d_model]\n","            gt: ground truth value, tensor with shape [batch, d_model]\n","        \"\"\"\n","        if self.norm_dim is not None:\n","            pred = F.normalize(pred, p=2, dim=self.norm_dim)\n","            gt = F.normalize(gt, p=2, dim=self.norm_dim)\n","\n","        squared_error = torch.square(pred - gt)     # squared_error = [batch, d_model]\n","        mse = torch.mean(squared_error)             # mse = [] (scalar tensor)\n","        return mse\n","\n","# ---- Semantic Concentration Loss ----\n","class SCL(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, heads):\n","        '''\n","        Compute the  dissimilarity between each head in each batch and then calculate the average across all batches.\n","        Args:\n","            heads = (batch, num_heads, seq_len, d_model)\n","        Return:\n","            Average cosine similarity between batches\n","        '''\n","        num_batches, num_heads, _, _ = heads.shape\n","        if num_heads <= 1:\n","            raise ValueError(\"There must be at least 2 heads to compute SCL.\")\n","\n","        loss = 0\n","        for batch in range(num_batches):\n","            batch_scl = count = 0\n","            for head1 in range(num_heads - 1):\n","                for head2 in range(head1 + 1, num_heads):\n","                    # Flatten the head tensors\n","                    head_tensor1 = heads[batch, head1].flatten()        # head_tensor1 = (seq_len * d_model)\n","                    head_tensor2 = heads[batch, head2].flatten()        # head_tensor2 = (seq_len * d_model)\n","\n","                    # Compute SCL loss for the current pair of heads\n","                    similarity = F.cosine_similarity(head_tensor1.unsqueeze(0), head_tensor2.unsqueeze(0))\n","                    scl = 1 - similarity\n","                    # Calculate the total Semantic Concentration Loss for the current batch\n","                    batch_scl += scl\n","                    count += 1\n","\n","            batch_scl /= count\n","            loss += batch_scl\n","\n","        loss /= num_batches\n","        return loss"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T12:05:20.691368Z","iopub.status.busy":"2024-03-11T12:05:20.691074Z","iopub.status.idle":"2024-03-11T12:05:20.710981Z","shell.execute_reply":"2024-03-11T12:05:20.710146Z","shell.execute_reply.started":"2024-03-11T12:05:20.691344Z"},"id":"QCXk4bjtElNP","trusted":true},"outputs":[],"source":["class KTHDataset(Dataset):\n","    def __init__(self, root_dir, N):\n","        '''\n","        root_dir: directory of the dataset\n","        N: number of past frames\n","        '''\n","        self.root_dir = root_dir\n","        self.N = N\n","        self.samples = []\n","\n","        #Init embedding model when create instance of dataset -> Avoid model retrieval loops\n","        model_name = 'google/vit-base-patch16-224'\n","        self.emb_model = ViTModel.from_pretrained(model_name)\n","        self.emb_processor = ViTImageProcessor.from_pretrained(model_name)\n","        self.emb_model.to(get_device_available())\n","\n","        # Iterate through each category folder and collect video paths\n","        for category in os.listdir(root_dir):\n","            category_folder = os.path.join(root_dir, category)\n","            if os.path.isdir(category_folder):\n","                for video_file in os.listdir(category_folder):\n","                    video_path = os.path.join(category_folder, video_file)\n","                    # Get the total number of frames of the current video\n","                    num_frames = self.get_total_frames(video_path)\n","                    # Add each possible sequence in this video\n","                    for i in range(num_frames - N):\n","                        self.samples.append((video_path, i))\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        video_path, start_frame = self.samples[idx]\n","\n","        # Get N+1 frames of the video_path, starting from the \"start_frame\" index\n","        frames = self.read_frames(video_path, start_frame, self.N + 1)\n","\n","        # Process each frame to have the shape [channels, width, height]\n","        processed_frames = [self.process_frame(frame) for frame in frames]\n","\n","        # Stack N frames for input data\n","        data = torch.stack(processed_frames[:-1])   # data = [N, channels, width, height]\n","\n","        # Output data (label) is the ViT processed features of the last frame\n","        last_frame = processed_frames[-1]                # last_frame = [channels, width, height]\n","        inputs = self.emb_processor(images=last_frame, return_tensors='pt')\n","        pixel_values = inputs.pixel_values.to(get_device_available())\n","        with torch.no_grad():\n","            label = self.emb_model(pixel_values)\n","            # Representation of the entire frame\n","            label = label.last_hidden_state.mean(dim=1)     # label = [1, 768]\n","            label = label.squeeze()                         # label = [768]\n","\n","        return data, label\n","\n","    def read_frames(self, video_path, start_frame, num_frames):\n","        frames = []\n","        try:\n","            reader = imageio.get_reader(video_path)\n","\n","            # Skipping to the start frame\n","            for _ in range(start_frame):\n","                _ = reader.get_next_data()\n","\n","            # Reading the required number of frames\n","            for _ in range(num_frames):\n","                frame = reader.get_next_data()\n","                frames.append(frame)\n","\n","        except Exception as e:\n","            print(f\"Error reading frames from {video_path}: {e}\")\n","\n","        return frames\n","\n","    def get_total_frames(self, video_path):\n","        num_frames = 0\n","        try:\n","            reader = imageio.get_reader(video_path)\n","            for _ in reader:\n","                num_frames += 1\n","        except Exception as e:\n","            print(f\"Error counting frames in video file {video_path}: {e}\")\n","\n","        return num_frames\n","\n","    def process_frame(self, frame):\n","        '''\n","        Process a frame to have the shape of [channels, width, height]\n","        Args:\n","            frame: the frame to be processed\n","        '''\n","        # Convert the frame to a PyTorch tensor\n","        frame_tensor = torch.from_numpy(frame)\n","\n","        # The frame is originally in (H, W, C) format => convert to (C, W, H)\n","        frame_tensor = frame_tensor.permute(2, 0, 1)\n","\n","        return frame_tensor\n","\n","def get_dataloader(dataset, batch_size, train_split=0.7, val_split=0.15, test_split=0.15, shuffle_train=True):\n","    \"\"\"\n","    Create DataLoaders for training, validation, and testing.\n","    Args:\n","        dataset: the processed dataset. Each data instance is a tuple of (data, label).\n","        batch_size: Batch size for the DataLoaders.\n","        train_split: Proportion of data to use for training.\n","        val_split: Proportion of data to use for validation.\n","        test_split: Proportion of data to use for testing.\n","        shuffle_train: Whether to shuffle the training dataset.\n","    return:\n","        A tuple of DataLoaders (train_loader, val_loader, test_loader).\n","    \"\"\"\n","    # Calculate the sizes of each split\n","    dataset_size = len(dataset)\n","    train_size = int(dataset_size * train_split)\n","    val_size = int(dataset_size * val_split)\n","    test_size = dataset_size - (train_size + val_size)\n","\n","    # Split the dataset\n","    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n","\n","    # Create DataLoaders for each split\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_train)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, val_loader, test_loader"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T12:05:20.712354Z","iopub.status.busy":"2024-03-11T12:05:20.712103Z","iopub.status.idle":"2024-03-11T12:05:20.722629Z","shell.execute_reply":"2024-03-11T12:05:20.721912Z","shell.execute_reply.started":"2024-03-11T12:05:20.712331Z"},"id":"7jIo-JIREoMl","trusted":true},"outputs":[],"source":["def get_device_available():\n","    ''' Detect available training device'''\n","    device = torch.device('cpu')\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","    elif torch.backends.mps.is_available():\n","        device = torch.device(\"mps\")\n","    return device\n","\n","def set_seed(seed):\n","    ''' Set random seed '''\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    device = get_device_available()\n","    if device == torch.device(\"mps\"):\n","        torch.mps.manual_seed(seed)\n","    elif device == torch.device(\"cuda\"):\n","        torch.cuda.manual_seed(seed)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T12:05:20.726494Z","iopub.status.busy":"2024-03-11T12:05:20.726241Z","iopub.status.idle":"2024-03-11T12:05:20.737129Z","shell.execute_reply":"2024-03-11T12:05:20.736264Z","shell.execute_reply.started":"2024-03-11T12:05:20.726471Z"},"id":"0c7JErLZE0o6","trusted":true},"outputs":[],"source":["class Loss_tuple(object):\n","    def __init__(self):\n","        self.train = []\n","        self.val = []\n","\n","def init_loss_dict(loss_name_list, history_loss_dict = None):\n","    loss_dict = {}\n","    for name in loss_name_list:\n","        loss_dict[name] = Loss_tuple()\n","    loss_dict['epochs'] = 0\n","\n","    if history_loss_dict is not None:\n","        for k, v in history_loss_dict.items():\n","            loss_dict[k] = v\n","\n","        for k, v in loss_dict.items():\n","            if k not in history_loss_dict:\n","                lt = Loss_tuple()\n","                lt.train = [0] * history_loss_dict['epochs']\n","                lt.val = [0] * history_loss_dict['epochs']\n","                loss_dict[k] = lt\n","\n","    return loss_dict\n","\n","def load_ckpt(ckpt_path, model, optimizer, loss_dict):\n","    ckpt = torch.load(ckpt_path)\n","\n","    # Retrieve the training parameters\n","    epoch = ckpt['epoch']\n","    loss_dict = ckpt['loss_dict']\n","    model_state_dict = ckpt['model_state_dict']\n","    optimizer_state_dict = ckpt['optimizer_state_dict']\n","\n","    return epoch, loss_dict, model_state_dict, optimizer_state_dict\n","\n","def save_ckpt(model, optimizer, epoch, loss_dict, save_dir):\n","    if not Path(save_dir).exists():\n","      Path(save_dir).mkdir(parents=True, exist_ok=True)\n","    ckpt_file = Path(save_dir).joinpath(f\"epoch_{epoch}.tar\")\n","\n","    model_state = model.state_dict()\n","    optimizer_state = optimizer.state_dict()\n","\n","    torch.save({\n","        'epoch': epoch,\n","        'loss_dict': loss_dict,\n","        'model_state_dict': model_state,\n","        'optimizer_state_dict': optimizer_state\n","    }, ckpt_file.absolute().as_posix())"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T12:05:20.739332Z","iopub.status.busy":"2024-03-11T12:05:20.738526Z","iopub.status.idle":"2024-03-11T12:05:20.751414Z","shell.execute_reply":"2024-03-11T12:05:20.750656Z","shell.execute_reply.started":"2024-03-11T12:05:20.739301Z"},"id":"9n2_IjlgI8zq","trusted":true},"outputs":[],"source":["def single_iter(model, optimizer, sample, device, task_loss, lambda_scl_loss=1, train_flag=True):\n","  '''\n","  Inputs:\n","  model: SCP model\n","  optimizer: Optimizer used\n","  sample: A single batch from loader\n","  device: Operating device (cuda/mps/cpu) -> Can be optimized by using get_available_device\n","  task_loss: Loss function for specific task\n","  lambda_scl_loss: Weight of SCL loss\n","  train_flag: True if training, False if evaluating\n","  '''\n","  #Get frames and label to the device\n","  past_frames, label = sample\n","  past_frames.to(device)\n","  label.to(device)\n","\n","  #Train phase\n","  if train_flag:\n","    #Reset gradients of model\n","    model.zero_grad(set_to_none=True)\n","    #Forward pass\n","    pred_frame, scl_loss = model(past_frames)\n","\n","    if optimizer is not None:\n","      #All parameters need to update have been set to require_grad = True earlier => Skip this modify step\n","      task_loss = task_loss(pred_frame, label)\n","      #Apply loss formula (total loss = task_loss + scl_loss * lambda)\n","      total_loss = task_loss + torch.mul(scl_loss, lambda_scl_loss)\n","      total_loss.backward()\n","      optimizer.step()\n","\n","  #Evaluate phase\n","  else:\n","    #Reset gradients of model\n","    model.zero_grad(set_to_none=True)\n","    #Forward pass\n","    pred_frame, scl_loss = model(past_frames)\n","\n","    if optimizer is not None:\n","      #All parameters need to update have been set to require_grad = True earlier => Skip this modify step\n","      task_loss = task_loss(pred_frame, label)\n","      #Apply loss formula (total loss = task_loss + scl_loss * lambda)\n","      total_loss = task_loss + torch.mul(scl_loss, lambda_scl_loss)\n","\n","  iter_loss_dict = {'Total': total_loss, 'MSE': task_loss, 'SCL': scl_loss}\n","  return iter_loss_dict\n","\n","def write_summary(summary_writer, loss_dict, train_flag=True):\n","  curr_loss = loss_dict.copy()\n","  if (train_flag):\n","    for k, v in curr_loss.items():\n","      #Exclude k = epochs when writing to tensorboard\n","      if (k != 'epochs'):\n","        for i in range(len(v.train)):\n","          summary_writer.add_scalars(k, {'train': v.train[i]}, i+1)\n","  else:\n","    for k, v in curr_loss.items():\n","      #Exclude k = epochs when writing to tensorboard\n","      if (k != 'epochs'):\n","        for i in range(len(v.val)):\n","          summary_writer.add_scalars(k, {'val': v.val[i]}, i+1)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T12:05:20.754361Z","iopub.status.busy":"2024-03-11T12:05:20.753939Z","iopub.status.idle":"2024-03-11T12:05:58.518830Z","shell.execute_reply":"2024-03-11T12:05:58.517657Z","shell.execute_reply.started":"2024-03-11T12:05:20.754337Z"},"id":"61t7KUVmFOlO","outputId":"1dd55f39-eb6c-49a3-e614-090b82efdc8d","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eea89771494e4be79f1c4d474278aafe","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d6d1b8ac66bf48379461c9566453fdeb","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9531877b8a04e1fa3fda5bdb4cafcec","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["\n","        set_seed(2023)\n","        ckpt_save_dir = Path.cwd().joinpath('checkpoint')\n","        tensorboard_save_dir = Path.cwd().joinpath('tensorboard')\n","        resume_ckpt = None\n","\n","        if not Path(ckpt_save_dir).exists():\n","                Path(ckpt_save_dir).mkdir(parents=True, exist_ok=True)\n","        logging.basicConfig(level=logging.INFO,\n","                        datefmt='%a, %d %b %Y %H:%M:%S',\n","                        format='[%(levelname)s] %(message)s - (%(filename)s)',      # e.g., [INFO] Log message 1 - (main.py)\n","                        filename=ckpt_save_dir.joinpath('train_log.log').absolute().as_posix(),\n","                        filemode='a')\n","\n","        start_epoch = 0\n","        summary_writer = SummaryWriter(tensorboard_save_dir.absolute().as_posix())\n","        num_past_frames = 10\n","        epochs = 10\n","        lr = 1e-4\n","        dropout = 0.1\n","        device = get_device_available()         # device = cuda/mps if available. Otherwise, device = cpu\n","\n","        ##################### Init Dataset ###########################\n","        root_dir = '/kaggle/input'\n","        seq_len = 5               # Number of previous frames\n","        batch_size = 32\n","        train_split = 0.001\n","        val_split = 0.0005\n","        test_split = 0.0005\n","        val_per_epoch = 4\n","        # Initialise the dataset\n","        full_dataset = KTHDataset(root_dir, seq_len)\n","        # Get the dataloader\n","        train_loader, val_loader, test_loader = get_dataloader(full_dataset, batch_size, train_split, val_split, test_split)\n","\n","        ##################### Init Loss Function ###########################\n","        loss_name_list = ['MSE', 'SCL', 'Total']\n","        loss_dict = init_loss_dict(loss_name_list)\n","        mse = MSE()\n","\n","        ##################### Resume training from checkpoint ###########################\n","        if resume_ckpt is not None:\n","                ckpt = torch.load(resume_ckpt)\n","                # Restore the model and optimizer state\n","                model.load_state_dict(ckpt['model_state_dict'])\n","                optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n","                # Load other training parameters\n","                start_epoch = ckpt[\"epoch\"]\n","                loss_dict = ckpt[\"loss_dict\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T12:05:58.521088Z","iopub.status.busy":"2024-03-11T12:05:58.520717Z"},"id":"Qg08wNXK9Aiw","outputId":"75b38d6f-f585-41d3-fe3b-6d0f07d996ea","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Total number of model's parameters: 219247872\n","{'Total': tensor([1.1392], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(1.0057, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1335], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.7853], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.6550, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1303], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.6462], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.5093, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1369], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5840], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4425, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1415], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5844], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4493, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1351], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5562], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4327, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1235], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5775], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4439, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1336], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5742], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4478, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1264], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5488], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4162, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1326], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5620], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4273, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1347], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5513], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4180, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1332], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5742], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4386, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1356], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5454], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4180, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1274], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5554], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4251, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1303], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5325], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4026, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1299], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5522], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4216, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1306], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5341], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4086, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1256], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5404], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4152, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1252], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5410], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4170, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1241], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5376], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4095, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1280], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5300], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4028, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1272], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5442], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4190, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1252], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5351], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4080, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1270], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5663], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4247, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1416], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5494], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4144, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1350], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5241], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4004, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1236], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5335], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4116, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1219], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5620], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4280, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1339], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5385], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4165, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1220], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5393], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4148, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1245], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5254], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4005, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1248], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5242], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.3925, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1317], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5431], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4164, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1267], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5415], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4120, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1295], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5233], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.3989, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1243], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5323], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.3905, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1418], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5375], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4078, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1297], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5427], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4156, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1271], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5326], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.4061, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1264], device='cuda:0', grad_fn=<AddBackward0>)}\n","{'Total': tensor([0.5273], device='cuda:0', grad_fn=<AddBackward0>), 'MSE': tensor(0.3982, device='cuda:0', grad_fn=<MeanBackward0>), 'SCL': tensor([0.1291], device='cuda:0', grad_fn=<AddBackward0>)}\n"]}],"source":["##################### Init Model ###########################\n","d_model = 768               # feature dimension of an input embedding\n","N = 6                       # Number of encoder blocks in the model\n","h = 8                       # Number of heads\n","model = build_model(d_model, seq_len, N, h, device=device)\n","optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr)\n","model_num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"Total number of model's parameters: {model_num_params}\")\n","\n","##################### Train ###########################\n","for epoch in range(start_epoch + 1, epochs +  1):\n","  loss_dict['epochs'] = epoch\n","  #Init temporary loss dict (Individual loss of each iteration)\n","  iters_loss_dict = init_loss_dict(loss_name_list)\n","\n","  #Get current date/time\n","  epoch_time = datetime.now()\n","  epoch_loss_dict = init_loss_dict(loss_name_list)\n","  #Train phase\n","  iter_count = len(train_loader.dataset)\n","  for idx, sample in enumerate(train_loader, 0):\n","    iter_loss_dict = single_iter(model, optimizer, sample, device, mse)\n","    #Add loss to epoch loss\n","    for k, v in iter_loss_dict.items():\n","      iters_loss_dict[k].train.append(iter_loss_dict[k])\n","  #Take average epoch loss\n","  for k, v in iters_loss_dict.items():\n","    if (k != 'epochs'):\n","      loss_concat = torch.stack(v.train)\n","      loss_dict[k].train.append(torch.mean(loss_concat))\n","  write_summary(summary_writer, loss_dict)\n","\n","  if (epoch % val_per_epoch == 0):\n","    #Evaluation phase\n","    for idx, sample in enumerate(val_loader, 0):\n","      iter_loss_dict = single_iter(model, optimizer, sample, device, mse, train_flag = False)\n","    #Add loss to epoch loss\n","      for k, v in iter_loss_dict.items():\n","        iters_loss_dict[k].val.append(iter_loss_dict[k])\n","    #Take average epoch loss\n","    for k, v in iters_loss_dict.items():\n","      if (k != 'epochs'):\n","        loss_concat = torch.stack(v.val)\n","        loss_dict[k].val.append(torch.mean(loss_concat))\n","    write_summary(summary_writer, loss_dict, train_flag=False)\n","\n","  #Save checkpoint\n","    \n","  epoch_time_used = datetime.now() - epoch_time\n","\n","  logging.info(f\"epoch {epoch}, {epoch_loss_dict['Total']}\")\n","  logging.info(f\"Estimated remaining training time: {epoch_time_used.total_seconds()/3600. * (start_epoch + epochs - epoch)} Hours\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!tensorboard --logdir=tensorboard"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4569935,"sourceId":7804104,"sourceType":"datasetVersion"},{"datasetId":4574569,"sourceId":7810426,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0f1c091822dd41e980d31b441ce0e273":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15e38dd889684b7e9168ddd2be33d869":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2410ecd7f69c4141b486a1b4ecefc368":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"41d70b363e7e494491da886599240166":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4518a624fe7b4d849c5fe9c670617669":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"457d3e071e364daf89f3751796c8a855":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57944eb180d44373bf800c2bc8229f4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59f80f1325854bfd9a0eb6c1afb7d0e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f0f41ed5b3941e2a95d696cf5c488b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66737c92fe9b4eec80424c5fb9dbfa69":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68a5eeca3e2e4d74a7efb01c2c4c630a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6989ecb103904551bbc0743c998e442a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae124cf6ca1c4d09a5600048ca08f9b8","IPY_MODEL_b8fb277f46ab49219d47d0c13c39d287","IPY_MODEL_f2caddc9c10f467f99f9ba8a9a34df62"],"layout":"IPY_MODEL_cb34a35e4abb4f09bbc4c0c1ecbc2156"}},"7a5999132a784fd2b518226cfeeb0a72":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fcc7e6f1197a418f8f5efcd4c608137a","IPY_MODEL_a757a0f5865d42c78d22d51e270be5b5","IPY_MODEL_d571f38729cd4943a1b003aea5f5c6fc"],"layout":"IPY_MODEL_66737c92fe9b4eec80424c5fb9dbfa69"}},"889596dc83f74561b2af874c3e5c7e4b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d504b923ba642ddbbe4260cb901c6ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c41b18b0dc54598b133b2e1180f4377":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a007548170a641ffb98d1d5f96519c85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1de2854914b47ffa20ce9f71e453023":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc1dc21fa9ac4bd1b49f4fbeae0e4dc8","IPY_MODEL_d9bfd2f234c8484897d4efe6a1b4b687","IPY_MODEL_e70783bef9eb418ea508474c7398e61b"],"layout":"IPY_MODEL_fd93338bc587464f96a285d9e5dad268"}},"a757a0f5865d42c78d22d51e270be5b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7baae289c5747939dcfe0d643e101d9","max":160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2410ecd7f69c4141b486a1b4ecefc368","value":160}},"aa7f4f4ed3ae42dc93652efee5ac82aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae124cf6ca1c4d09a5600048ca08f9b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa7f4f4ed3ae42dc93652efee5ac82aa","placeholder":"​","style":"IPY_MODEL_57944eb180d44373bf800c2bc8229f4c","value":"model.safetensors: 100%"}},"b8fb277f46ab49219d47d0c13c39d287":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_889596dc83f74561b2af874c3e5c7e4b","max":346293852,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bb53447928304ccc8c08989a53fa5f6f","value":346293852}},"bb53447928304ccc8c08989a53fa5f6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bc1dc21fa9ac4bd1b49f4fbeae0e4dc8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68a5eeca3e2e4d74a7efb01c2c4c630a","placeholder":"​","style":"IPY_MODEL_a007548170a641ffb98d1d5f96519c85","value":"config.json: 100%"}},"c5e4007b22454341bd2695c5ac030391":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb34a35e4abb4f09bbc4c0c1ecbc2156":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d571f38729cd4943a1b003aea5f5c6fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4518a624fe7b4d849c5fe9c670617669","placeholder":"​","style":"IPY_MODEL_0f1c091822dd41e980d31b441ce0e273","value":" 160/160 [00:00&lt;00:00, 2.03kB/s]"}},"d9bfd2f234c8484897d4efe6a1b4b687":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d504b923ba642ddbbe4260cb901c6ea","max":69665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_457d3e071e364daf89f3751796c8a855","value":69665}},"e70783bef9eb418ea508474c7398e61b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c41b18b0dc54598b133b2e1180f4377","placeholder":"​","style":"IPY_MODEL_41d70b363e7e494491da886599240166","value":" 69.7k/69.7k [00:00&lt;00:00, 1.53MB/s]"}},"e7baae289c5747939dcfe0d643e101d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2caddc9c10f467f99f9ba8a9a34df62":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5e4007b22454341bd2695c5ac030391","placeholder":"​","style":"IPY_MODEL_15e38dd889684b7e9168ddd2be33d869","value":" 346M/346M [00:05&lt;00:00, 68.9MB/s]"}},"fcc7e6f1197a418f8f5efcd4c608137a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59f80f1325854bfd9a0eb6c1afb7d0e9","placeholder":"​","style":"IPY_MODEL_5f0f41ed5b3941e2a95d696cf5c488b2","value":"preprocessor_config.json: 100%"}},"fd93338bc587464f96a285d9e5dad268":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":4}
